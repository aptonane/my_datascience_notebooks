{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "## Using stemming and lemmatization in our vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anapaulavarejao/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/anapaulavarejao/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "from time import time\n",
    "import pandas as pd\n",
    "pd.set_option(\"max.colwidth\", 500)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\r\\n\\r\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ing...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad reviews about this place. It goes to show you, you can please everyone. They are probably griping about something that their own fault...there are many people like that.\\r\\n\\r\\nIn any case, my friend and I arrived at about 5:50 PM this past Sunday. It was pretty crowded, more than I thought for a Sunday evening and thought we would have to wait forever to get a seat but they said we'll be seated when the girl comes back from seating someone else. We we...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I also dig their candy selection :)</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of paths, a desert xeriscape, baseball fields, ballparks, and a lake with ducks.\\r\\n\\r\\nThe Scottsdale Park and Rec Dept. does a wonderful job of keeping the park clean and shaded.  You can find trash cans and poopy-pick up mitts located all over the park and paths.\\r\\n\\r\\nThe fenced in area is huge to let the dogs run, play, and sniff!</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!! Not to go into detail, but let me assure you if you have any issues (albeit rare) speak with Scott and treat the guy with some respect as you state your case and I'd be surprised if you don't walk out totally satisfied as I just did. Like I always say..... \"Mistakes are inevitable, it's how we recover from them that is important\"!!!\\r\\n\\r\\nThanks to Scott and his awesome staff. You've got a customer for life!! .......... :^)</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  text  \\\n",
       "0  My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\r\\n\\r\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ing...   \n",
       "1  I have no idea why some people give bad reviews about this place. It goes to show you, you can please everyone. They are probably griping about something that their own fault...there are many people like that.\\r\\n\\r\\nIn any case, my friend and I arrived at about 5:50 PM this past Sunday. It was pretty crowded, more than I thought for a Sunday evening and thought we would have to wait forever to get a seat but they said we'll be seated when the girl comes back from seating someone else. We we...   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                         love the gyro plate. Rice is so good and I also dig their candy selection :)   \n",
       "3                                                                      Rosie, Dakota, and I LOVE Chaparral Dog Park!!! It's very convenient and surrounded by a lot of paths, a desert xeriscape, baseball fields, ballparks, and a lake with ducks.\\r\\n\\r\\nThe Scottsdale Park and Rec Dept. does a wonderful job of keeping the park clean and shaded.  You can find trash cans and poopy-pick up mitts located all over the park and paths.\\r\\n\\r\\nThe fenced in area is huge to let the dogs run, play, and sniff!   \n",
       "4                          General Manager Scott Petello is a good egg!!! Not to go into detail, but let me assure you if you have any issues (albeit rare) speak with Scott and treat the guy with some respect as you state your case and I'd be surprised if you don't walk out totally satisfied as I just did. Like I always say..... \"Mistakes are inevitable, it's how we recover from them that is important\"!!!\\r\\n\\r\\nThanks to Scott and his awesome staff. You've got a customer for life!! .......... :^)   \n",
       "\n",
       "     type                 user_id  cool  useful  funny  \n",
       "0  review  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  review  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  review  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  review  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  review  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in yelp review data\n",
    "path = \"yelp.csv\"\n",
    "yelp = pd.read_csv(path, encoding='unicode-escape')\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame called yelp_best_worst that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    0.816691\n",
      "1    0.183309\n",
      "Name: stars, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# define X and y\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "#Null accuracy\n",
    "print y.value_counts(normalize=True)\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the analyzer section of the CountVectorizer doc strings\n",
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The analyzer argument allows us to upload our function to transform/tokenize the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of stems\n",
    "def word_tokenize_stem(text):\n",
    "    #Transform and tokenize words using TextBlob\n",
    "    words = TextBlob(text).words\n",
    "    #Initialize stemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    #Return a list of the stems\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of lemons (noun version)\n",
    "def word_tokenize_lemma(text):\n",
    "    #Transform and tokenize words using TextBlob\n",
    "    words = TextBlob(text).words\n",
    "    #Return a list of lemons\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of lemons (verb version)\n",
    "def word_tokenize_lemma_verb(text):\n",
    "    words = TextBlob(text).words\n",
    "    #Return a list of lemons    \n",
    "    return [word.lemmatize(pos=\"v\") for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_model_evaluator(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    print \"Features: \", X_train_dtm.shape[1]\n",
    "    print \"Training Score: \", nb.score(X_train_dtm, y_train)\n",
    "    print \"Testing Score: \", nb.score(X_test_dtm, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions:\n",
    "** Word tokenization**: \n",
    "Separate text into units such as sentences or words, gives structure to previously unstructured text. It is relatively easy with English language text, not easy with some languages.\n",
    "** rules of tokenization**:\n",
    "The text is first tokenized into sentences using the PunktSentenceTokenizer. Then each sentence is tokenized into words using 4 different word tokenizers:\n",
    "- TreebankWordTokenizer\n",
    "- WordPunctTokenizer\n",
    "- PunctWordTokenizer\n",
    "- WhitespaceTokenizer\n",
    "\n",
    "**Stemming and lemmatization**\n",
    "**Stemming**: Reduce a word to its base/stem/root form. Often makes sense to treat related words the same way\n",
    "Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "Some search engines treat words with the same stem as synonyms\n",
    "\n",
    "**Lemmatization**: Derive the canonical form ('lemma') of a word. Can be better than stemming\n",
    "Notes: Uses a dictionary-based approach (slower than stemming)\n",
    "\n",
    "**Term Frequency-Inverse Document Frequency (TF-IDF)**: Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  13273\n",
      "Training Score:  0.970626631854\n",
      "Testing Score:  0.924657534247\n"
     ]
    }
   ],
   "source": [
    "#Initialize Count Vectorizer with stop_words set to english and analyzer to word_tokenize_stem\n",
    "vect = CountVectorizer(stop_words=\"english\", analyzer=word_tokenize_stem)\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  20599\n",
      "Training Score:  0.974216710183\n",
      "Testing Score:  0.904109589041\n"
     ]
    }
   ],
   "source": [
    "#Intialize Count Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma\n",
    "vect = CountVectorizer(stop_words=\"english\", analyzer=word_tokenize_lemma)\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  19431\n",
      "Training Score:  0.974216710183\n",
      "Testing Score:  0.906066536204\n"
     ]
    }
   ],
   "source": [
    "#Intialize Count Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma_verb\n",
    "vect = CountVectorizer(stop_words=\"english\", analyzer=word_tokenize_lemma_verb)\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  13273\n",
      "Training Score:  0.816906005222\n",
      "Testing Score:  0.819960861057\n"
     ]
    }
   ],
   "source": [
    "#Intialize Tfidf Vectorizer with stop_words set to english and analyzer to word_tokenize_stem\n",
    "vect = TfidfVectorizer(stop_words=\"english\", analyzer=word_tokenize_stem)\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  20599\n",
      "Training Score:  0.817232375979\n",
      "Testing Score:  0.819960861057\n"
     ]
    }
   ],
   "source": [
    "#Intialize Tfidf Vectorizer with stop_words set to english and analyzer to word_tokenize_lemma\n",
    "vect = TfidfVectorizer(stop_words=\"english\", analyzer=word_tokenize_lemma)\n",
    "#Pass vectorizer into function\n",
    "text_model_evaluator(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count - randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make pipeline for countvectorizer and naive bayes model\n",
    "pipe_cv = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "#Initialize parameters for count vectorizer\n",
    "param_grid_cv = {}\n",
    "param_grid_cv[\"countvectorizer__max_features\"] = [1000, 2500 ,5000, 7500,10000]\n",
    "param_grid_cv[\"countvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "param_grid_cv[\"countvectorizer__lowercase\"] = [True, False]\n",
    "param_grid_cv[\"countvectorizer__binary\"] = [True, False]\n",
    "param_grid_cv[\"countvectorizer__analyzer\"] = [\"word\", word_tokenize_stem,\n",
    "                                              word_tokenize_lemma, word_tokenize_lemma_verb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "460.15984416\n"
     ]
    }
   ],
   "source": [
    "#Randomized grid search with n_iter = 5\n",
    "randsearch_cv = RandomizedSearchCV(pipe_cv, n_iter = 5,\n",
    "                        param_distributions = param_grid_cv, cv = 5, scoring = \"accuracy\")\n",
    "#Time the code \n",
    "t = time()\n",
    "#Fit grid on data\n",
    "randsearch_cv.fit(X, y)\n",
    "#Print time difference\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'countvectorizer__lowercase': True, 'countvectorizer__analyzer': <function word_tokenize_lemma at 0x118d0a500>, 'countvectorizer__ngram_range': (1, 1), 'countvectorizer__binary': True, 'countvectorizer__max_features': 5000}\n",
      "0.928781204112\n"
     ]
    }
   ],
   "source": [
    "#Best params\n",
    "print randsearch_cv.best_params_\n",
    "#Best score\n",
    "print randsearch_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF: randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make pipeline for tfidfvectorizer and naive bayes model\n",
    "pipe_tf = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "\n",
    "#Intialize parameters for tfidf vectorizer\n",
    "param_grid_tf = {}\n",
    "param_grid_tf[\"tfidfvectorizer__max_features\"] = [1000, 2500 ,5000, 7500,10000]\n",
    "param_grid_tf[\"tfidfvectorizer__ngram_range\"] = [(1,1), (1,2), (2,2)]\n",
    "param_grid_tf[\"tfidfvectorizer__lowercase\"] = [True, False]\n",
    "param_grid_tf[\"tfidfvectorizer__binary\"] = [True, False]\n",
    "param_grid_tf[\"tfidfvectorizer__analyzer\"] = [\"word\", word_tokenize_stem,\n",
    "                                              word_tokenize_lemma, word_tokenize_lemma_verb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2338.76286483\n"
     ]
    }
   ],
   "source": [
    "#Randomized grid search with n_iter = 10\n",
    "randsearch_tf = RandomizedSearchCV(pipe_tf, n_iter = 10,\n",
    "                        param_distributions = param_grid_tf, cv = 5, scoring = \"accuracy\")\n",
    "#Time the code \n",
    "t = time()\n",
    "#Fit grid on data\n",
    "randsearch_tf.fit(X, y)\n",
    "#Print time difference\n",
    "print time() - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tfidfvectorizer__analyzer': <function word_tokenize_stem at 0x118d0a398>, 'tfidfvectorizer__ngram_range': (2, 2), 'tfidfvectorizer__lowercase': False, 'tfidfvectorizer__binary': True, 'tfidfvectorizer__max_features': 1000}\n",
      "0.868575624082\n"
     ]
    }
   ],
   "source": [
    "#Best params\n",
    "print randsearch_tf.best_params_\n",
    "#Best score\n",
    "print randsearch_tf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Accuracy = randsearch_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.83      0.91      0.87       184\n",
      "          5       0.98      0.96      0.97       838\n",
      "\n",
      "avg / total       0.95      0.95      0.95      1022\n",
      "\n",
      "[[167  17]\n",
      " [ 35 803]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = randsearch_cv.predict(X_test)\n",
    "print classification_report(y_test, y_pred)\n",
    "print confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          pred (1)     pred (5) total\n",
      "1      TN       76  FP      108   184\n",
      "5      FN        3  TP      835   838\n",
      "total           79          943  1022\n",
      "\t\n",
      "Sensitivity or Recall or True Positive Rate - TPR: 99.64 %\n",
      "Specificity or True Negative Rate - TNR: 41.3 %\n",
      "False Positive Rate - FPR: 58.7 %\n",
      "False Negative Rate - FNR: 0.36 %\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "tot_a_1 = TN + FP\n",
    "tot_a_2 = FN + TP\n",
    "tot_p_1 = TN + FN\n",
    "tot_p_2 = FP + TP\n",
    "tot_g = tot_a_1+tot_a_2\n",
    "mx =np.array([\"TN\",TN,\"FP\",FP,tot_a_1,\"FN\",FN,\"TP\", TP, tot_a_2,\"\", tot_p_1,\"\", tot_p_2, tot_g]).reshape(3,5)\n",
    "cols = [\" \",\"pred (1)\", \" \", \"pred (5)\",  \"total\"]\n",
    "index = [\"1\", \"5\", \"total\"]\n",
    "cm_df = pd.DataFrame(mx, columns=cols, index=index )\n",
    "print cm_df\n",
    "print \"\\t\"\n",
    "tpr = round(TP / float(TP + FN),4)\n",
    "tnr = round(TN / float(TN + FP),4)\n",
    "fpr = round(FP / float(TN + FP),4)\n",
    "fnr = round(FN / float(FN + TP),4)\n",
    "print \"Sensitivity or Recall or True Positive Rate - TPR:\", tpr*100, \"%\"\n",
    "print \"Specificity or True Negative Rate - TNR:\", tnr*100,\"%\"\n",
    "print \"False Positive Rate - FPR:\", fpr*100,\"%\"\n",
    "print \"False Negative Rate - FNR:\", fnr*100,\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
